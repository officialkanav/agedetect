{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ageclassification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit2843/agedetect/blob/master/ageclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_l9nbnVOHSKm",
        "colab_type": "code",
        "outputId": "fea3b88e-c95a-4a5b-dd64-e2db5c0a071f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install https://download.pytorch.org/whl/cu90/torch-1.0.0-cp36-cp36m-win_amd64.whl\n",
        "!pip3 install torchvision\n",
        "!git clone https://github.com/sanchit2843/agedetect\n",
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skimage import io, transform\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import cv2\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mtorch-1.0.0-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/e3/217dfd0834a51418c602c96b110059c477260c7fee898542b100913947cf/Pillow-5.4.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting torch (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 25kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x61930000 @  0x7f559d18a2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Installing collected packages: pillow, torch, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.0 torch-1.0.0 torchvision-0.2.1\n",
            "Cloning into 'agedetect'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 85 (delta 2), reused 9 (delta 2), pack-reused 75\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6q5InX1jrdhs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, with_statement, division\n",
        "import torch\n",
        "from tqdm.autonotebook import tqdm\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class LRFinder(object):\n",
        "    \"\"\"Learning rate range test.\n",
        "    The learning rate range test increases the learning rate in a pre-training run\n",
        "    between two boundaries in a linear or exponential manner. It provides valuable\n",
        "    information on how well the network can be trained over a range of learning rates\n",
        "    and what is the optimal learning rate.\n",
        "    Arguments:\n",
        "        model (torch.nn.Module): wrapped model.\n",
        "        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n",
        "            is assumed to be the lower boundary of the range test.\n",
        "        criterion (torch.nn.Module): wrapped loss function.\n",
        "        device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n",
        "            optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n",
        "            Alternatively, can be an object representing the device on which the\n",
        "            computation will take place. Default: None, uses the same device as `model`.\n",
        "    Example:\n",
        "        >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
        "        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
        "    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
        "    fastai/lr_find: https://github.com/fastai/fastai\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, criterion, device=None):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.history = {\"lr\": [], \"loss\": []}\n",
        "        self.best_loss = None\n",
        "\n",
        "        # Save the original state of the model and optimizer so they can be restored if\n",
        "        # needed\n",
        "        self.model_state = model.state_dict()\n",
        "        self.model_device = next(self.model.parameters()).device\n",
        "        self.optimizer_state = optimizer.state_dict()\n",
        "\n",
        "        # If device is None, use the same as the model\n",
        "        if device:\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.device = self.model_device\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n",
        "        self.model.load_state_dict(self.model_state)\n",
        "        self.model.to(self.model_device)\n",
        "        self.optimizer.load_state_dict(self.optimizer_state)\n",
        "\n",
        "    def range_test(\n",
        "        self,\n",
        "        train_loader,\n",
        "        val_loader=None,\n",
        "        end_lr=10,\n",
        "        num_iter=100,\n",
        "        step_mode=\"exp\",\n",
        "        smooth_f=0.05,\n",
        "        diverge_th=5,\n",
        "    ):\n",
        "        \"\"\"Performs the learning rate range test.\n",
        "        Arguments:\n",
        "            train_loader (torch.utils.data.DataLoader): the training set data laoder.\n",
        "            val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n",
        "                will only use the training loss. When given a data loader, the model is\n",
        "                evaluated after each iteration on that dataset and the evaluation loss\n",
        "                is used. Note that in this mode the test takes significantly longer but\n",
        "                generally produces more precise results. Default: None.\n",
        "            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n",
        "            num_iter (int, optional): the number of iterations over which the test\n",
        "                occurs. Default: 100.\n",
        "            step_mode (str, optional): one of the available learning rate policies,\n",
        "                linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n",
        "            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n",
        "                interval. Disabled if set to 0, otherwise the loss is smoothed using\n",
        "                exponential smoothing. Default: 0.05.\n",
        "            diverge_th (int, optional): the test is stopped when the loss surpasses the\n",
        "                threshold:  diverge_th * best_loss. Default: 5.\n",
        "        \"\"\"\n",
        "        # Reset test results\n",
        "        self.history = {\"lr\": [], \"loss\": []}\n",
        "        self.best_loss = None\n",
        "\n",
        "        # Move the model to the proper device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize the proper learning rate policy\n",
        "        if step_mode.lower() == \"exp\":\n",
        "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "        elif step_mode.lower() == \"linear\":\n",
        "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
        "        else:\n",
        "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
        "\n",
        "        if smooth_f < 0 or smooth_f >= 1:\n",
        "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
        "\n",
        "        # Create an iterator to get data batch by batch\n",
        "        iterator = iter(train_loader)\n",
        "        for iteration in tqdm(range(num_iter)):\n",
        "            # Get a new set of inputs and labels\n",
        "            try:\n",
        "                inputs, labels = next(iterator)\n",
        "            except StopIteration:\n",
        "                iterator = iter(train_loader)\n",
        "                inputs, labels = next(iterator)\n",
        "\n",
        "            # Train on batch and retrieve loss\n",
        "            loss = self._train_batch(inputs, labels)\n",
        "            if val_loader:\n",
        "                loss = self._validate(val_loader)\n",
        "\n",
        "            # Update the learning rate\n",
        "            lr_schedule.step()\n",
        "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
        "\n",
        "            # Track the best loss and smooth it if smooth_f is specified\n",
        "            if iteration == 0:\n",
        "                self.best_loss = loss\n",
        "            else:\n",
        "                if smooth_f > 0:\n",
        "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
        "                if loss < self.best_loss:\n",
        "                    self.best_loss = loss\n",
        "\n",
        "            # Check if the loss has diverged; if it has, stop the test\n",
        "            self.history[\"loss\"].append(loss)\n",
        "            if loss > diverge_th * self.best_loss:\n",
        "                print(\"Stopping early, the loss has diverged\")\n",
        "                break\n",
        "\n",
        "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
        "\n",
        "    def _train_batch(self, inputs, labels):\n",
        "        # Set model to training mode\n",
        "        self.model.train()\n",
        "\n",
        "        # Move data to the correct device\n",
        "        inputs = inputs.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "        labels = labels.type(torch.cuda.FloatTensor)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def _validate(self, dataloader):\n",
        "        # Set model to evaluation mode and disable gradient computation\n",
        "        running_loss = 0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                # Move data to the correct device\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # Forward pass and loss computation\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        return running_loss / len(dataloader.dataset)\n",
        "\n",
        "    def plot(self, skip_start=10, skip_end=5, log_lr=True):\n",
        "        \"\"\"Plots the learning rate range test.\n",
        "        Arguments:\n",
        "            skip_start (int, optional): number of batches to trim from the start.\n",
        "                Default: 10.\n",
        "            skip_end (int, optional): number of batches to trim from the start.\n",
        "                Default: 5.\n",
        "            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
        "                scale; otherwise, plotted in a linear scale. Default: True.\n",
        "        \"\"\"\n",
        "\n",
        "        if skip_start < 0:\n",
        "            raise ValueError(\"skip_start cannot be negative\")\n",
        "        if skip_end < 0:\n",
        "            raise ValueError(\"skip_end cannot be negative\")\n",
        "\n",
        "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
        "        # properly so the behaviour is the expected\n",
        "        lrs = self.history[\"lr\"]\n",
        "        losses = self.history[\"loss\"]\n",
        "        if skip_end == 0:\n",
        "            lrs = lrs[skip_start:]\n",
        "            losses = losses[skip_start:]\n",
        "        else:\n",
        "            lrs = lrs[skip_start:-skip_end]\n",
        "            losses = losses[skip_start:-skip_end]\n",
        "\n",
        "        # Plot loss as a function of the learning rate\n",
        "        plt.plot(lrs, losses)\n",
        "        if log_lr:\n",
        "            plt.xscale(\"log\")\n",
        "        plt.xlabel(\"Learning rate\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class LinearLR(_LRScheduler):\n",
        "    \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
        "    iterations.\n",
        "    Arguments:\n",
        "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
        "        end_lr (float, optional): the initial learning rate which is the lower\n",
        "            boundary of the test. Default: 10.\n",
        "        num_iter (int, optional): the number of iterations over which the test\n",
        "            occurs. Default: 100.\n",
        "        last_epoch (int): the index of last epoch. Default: -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "        self.end_lr = end_lr\n",
        "        self.num_iter = num_iter\n",
        "        super(LinearLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        curr_iter = self.last_epoch + 1\n",
        "        r = curr_iter / self.num_iter\n",
        "        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class ExponentialLR(_LRScheduler):\n",
        "    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
        "    iterations.\n",
        "    Arguments:\n",
        "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
        "        end_lr (float, optional): the initial learning rate which is the lower\n",
        "            boundary of the test. Default: 10.\n",
        "        num_iter (int, optional): the number of iterations over which the test\n",
        "            occurs. Default: 100.\n",
        "        last_epoch (int): the index of last epoch. Default: -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "        self.end_lr = end_lr\n",
        "        self.num_iter = num_iter\n",
        "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        curr_iter = self.last_epoch + 1\n",
        "        r = curr_iter / self.num_iter\n",
        "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NHmgIsnWHTrM",
        "colab_type": "code",
        "outputId": "6693980c-3b73-4958-a627-06974cce3b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('./pytorch-lr-finder'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['README.md', 'examples', 'requirements.txt', '.git', 'images', '.gitignore', 'LICENSE', 'lr_finder.py']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dnNGSgdnrt1k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xDdbO071IXKD",
        "colab_type": "code",
        "outputId": "f06732ad-e02d-49ef-caaa-b66e226dc489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install googledrivedownloader\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='0BxYys69jI14kYVM3aVhKS1VhRUk',\n",
        "                                    dest_path='./UTKFace.tar.gz')\n",
        "import tarfile\n",
        "tar = tarfile.open(\"./UTKFace.tar.gz\")\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(categorical_features = [0])\n",
        "class age_dataset(Dataset):\n",
        "  def __init__(self,label,image_dir,transform = None):\n",
        "    self.data = pd.read_csv(label)\n",
        "    self.img_dir = image_dir\n",
        "    self.transform = transform\n",
        "    self.id = self.data.iloc[:,1:2].values\n",
        "  def __len__(self):\n",
        "        return len(self.id)\n",
        "  def __getitem__(self,idx):\n",
        "    img_name = os.path.join(self.img_dir,self.id[idx,0])\n",
        "    image = cv2.imread(img_name)\n",
        "    age1 = self.data.iloc[:,2:3].values\n",
        "#    age1 = ohe.fit_transform(age1).toarray()\n",
        "    age = age1[idx,:]\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image,age \n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.39125082, 0.45353994, 0.5837345 ], [0.20129617, 0.21080811, 0.24163313])\n",
        "    ])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.39125082, 0.45353994, 0.5837345 ], [0.20129617, 0.21080811, 0.24163313])\n",
        "    ])\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "age_data = age_dataset('./agedetect/label.csv','./UTKFace')\n",
        "\n",
        "test_size = 0.2\n",
        "valid_size = 0.1\n",
        "\n",
        "data_len = len(age_data)\n",
        "indices = list(range(data_len))\n",
        "\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "split1 = int(np.floor(valid_size * data_len))\n",
        "split2 = int(np.floor(test_size * data_len))\n",
        "split2 = split2+split1\n",
        "\n",
        "valid_idx , test_idx, train_idx = indices[:split1], indices[split1:split2] , indices[split2:]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "train_loader = DataLoader(age_data, batch_size=32, sampler=train_sampler)\n",
        "valid_loader = DataLoader(age_data, batch_size=32 , sampler=valid_sampler)\n",
        "test_loader = DataLoader(age_data, batch_size=32 , sampler=test_sampler)\n",
        "\n",
        "dataloaders = {'train':train_loader,'val':valid_loader}\n",
        "\n",
        "dataset_sizes = {'train':16597,'test':4741,'val':2370}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (0.3)\n",
            "Downloading 0BxYys69jI14kYVM3aVhKS1VhRUk into ./UTKFace.tar.gz... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kwD4PBSFI4G9",
        "colab_type": "code",
        "outputId": "0ec4c6cb-22fc-4715-c7a7-4d3fb968c3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "num_ftrs = resnet50.fc.in_features\n",
        "resnet50.fc = nn.Sequential(nn.Linear(num_ftrs,256),nn.ReLU(),nn.Linear(256,104),nn.Softmax())\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format('./runs')\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
        "\n",
        "resnet50 = resnet50.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.Adam(resnet50.parameters(), lr=1)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/lib/python3.6/json/__init__.py\", line 299, in load\n",
            "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
            "  File \"/usr/lib/python3.6/json/__init__.py\", line 354, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.6/json/decoder.py\", line 339, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DwjYIdCgGldm",
        "colab_type": "code",
        "outputId": "b1da38dc-771d-4652-d838-55cfad011511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboardx\n",
        "from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == ' rain':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                inputs = inputs.type(torch.cuda.FloatTensor)\n",
        "                labels = labels.to(device)\n",
        "                labels = labels.type(torch.cuda.FloatTensor)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    \n",
        "                    outputs = model(inputs1)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    preds = torch.reshape(preds,(1))\n",
        "                    #print(outputs.shape,labels.shape)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, labels = torch.max(labels,1)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics \n",
        "                running_loss += loss.item() * inputs1.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            writer.add_scalar('./sanchit/scalar1', epoch_acc , epoch)\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(),'./classifier{}'.format(best_acc))\n",
        "    \n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.14.6)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (3.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardx) (40.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CSzyqseP_ssb",
        "colab_type": "code",
        "outputId": "8fceea35-62b5-49dd-8b9e-7b5e22fb67a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "model_ft = train_model(resnet50, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/2\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8b750f732436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-53207c677879>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'inputs1' is not defined"
          ]
        }
      ]
    }
  ]
}